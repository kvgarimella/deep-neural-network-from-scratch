{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "In this notebook, we will be building a neural network from scratch. It's going to be a simple neural network with only forward weights. With that respect, it will be a *feed-forward* neural network. Most of this material will be adapted from the book, [Learning from Data](http://amlbook.com/). \n",
    "\n",
    "## The Structure\n",
    "Picture from Learning From Data, Chapter 7 by AML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the notation down. First, the neural network above has **three** layers. We don't count the input layer, because it is usually just the raw data. Each layer has a certain number of nodes. For example, the $l=2$ layer above has $3$ nodes. In general, layer $l$ will have $d^{(l)} + 1$ nodes. The $+1$ results from the bias node. Now, let's name each node. For example, the last node on layer $l=0$ can be written as $x^{(0)}_{d^{(0)}}$. We also need to define the weights. $w^{(l)}_{ij}$ refers to the weight that connects $x^{(l-1)}_{i}$ to $x^{(l)}_{j}$. So summarizing this all in bullets\n",
    "* $x^{(l)}_{i}$ is the *i*th node from layer $l$\n",
    "* Each layer $l$ has $d^{(l)} + 1$ nodes\n",
    "\n",
    "The following our bounds on $w^{(l)}_{ij}$, the weight that connects $x^{(l-1)}_{i}$ to $x^{(l)}_{j}$:\n",
    "* $0 \\le i \\le d^{(l-1)}$\n",
    "* $1 \\le j \\le d^{(l)}$\n",
    "* $0 \\le l \\le L$\n",
    "\n",
    "Notice how $j$ is lower bounded by $1$. In the above diagram, no weights are going into the bias nodes. This is convention we will use for our deep neural network.\n",
    "## Imports ##\n",
    "We will only be using *numpy* for vectorization and perhaps *matplotlib* for plotting. We will code forward and back propogation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST DATA ##\n",
    "We will be making use of the mnist hand-written digits dataset. You can run ** pip install mnist** to get this package. The following cell block will load the train/test images and associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 255\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(train_images.max(), test_images.max())\n",
    "print(train_images.min(), test_images.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to normalize these images so that our data points are strictly between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images / train_images.max()\n",
    "test_images  = test_images  / test_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 60,000 training images and 10,000 test images. Let's plot one of the training images just to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for train_images[1000]: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13b999400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b8e26a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Label for train_images[1000]: %d\" % train_labels[1000])\n",
    "plt.imshow(train_images[1000], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see that the image is clearly a zero and the label matches. Let's analyze one training image a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[1000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a 28 by 28 grid of pixel values between 0 and 1. I would like this to be the input to our deep neural network. Therefore, our zeroth layer will have 785 nodes (28 x 28 + 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network will have 784, 16, 16, and 10 nodes in each layer (excluding the bias node except for the last layer). The last layer has 10 nodes, each node representing the probability that the input images corresponds to that digit. To squeeze a function into a probabalistic model, we will use the sigmoid function for the last layer and $tanh$ function for all other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "d = {0:784, 1:16, 2:16, 3:10}\n",
    "num_layers = len(d) - 1\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will be using $tanh(x)$ as the activation functions for all but the last layer. It has some nice properties (ranges from -1 to 1 and is differentiable). Also the derivative of $tanh(x)$ is $1 - tanh(x)^2$. Recall to set the weights to small random numbers as our function is not convex; we don't want to get stuck at a local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W       = {}\n",
    "S       = {}\n",
    "X       = {}\n",
    "deltas  = {}  \n",
    "grads   = {}\n",
    "for l in range(1,len(d)):\n",
    "    W[l]      = np.random.normal(0,0.1,(d[l-1] + 1, d[l]))\n",
    "    grads[l]  = np.zeros((d[l-1] + 1, d[l]))\n",
    "    S[l]      = np.zeros((d[l],1))\n",
    "    X[l]      = np.ones((d[l] + 1,1))\n",
    "    deltas[l] = np.zeros((d[l],1))\n",
    "X[0] = np.ones((d[0] + 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    feed_forward(x, W, S)\n",
    "    return X[num_layers][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def theta_prime(x):\n",
    "    return 1 - np.tanh(x)*np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(x, W, S):\n",
    "    X[0] = x.reshape(X[0].shape[0], 1)\n",
    "    for l in range(1, len(d)):\n",
    "        S[l] = np.matmul(W[l].T, X[l-1]).reshape(S[l].shape[0], 1)\n",
    "#         X[l][1:] = sigmoid(S[l])\n",
    "        if l == num_layers:\n",
    "            X[l][1:] = sigmoid(S[l])\n",
    "        else:\n",
    "            X[l][1:] = theta(S[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Prop ###\n",
    "The minimizer of deep neural networks. Here, we are using dynamic programming to speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(deltas, X, S):\n",
    "    for l in range(num_layers - 1, 0, -1):\n",
    "        deltas[l] = sigmoid_prime(S[l]) * np.matmul(W[l + 1][1:], deltas[l+1])\n",
    "        grads[l]  = np.matmul(X[l-1], deltas[l].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent ###\n",
    "\n",
    "We will put feed forward and back prop into our stochastic grad descent. We use stochastic to decrease overall runtime and help us avoid being trapped in local extrema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StochasticGradientDescent(W, S, deltas, eta, iterations):\n",
    "    num = iterations // 2\n",
    "    for i in range(iterations):\n",
    "        if i == num:\n",
    "            print(\"halfway through.\")\n",
    "        idx = np.random.randint(0, train_images.shape[0])\n",
    "        x = np.append([1], train_images[idx].flatten())\n",
    "        curr_y = np.zeros((10,1))\n",
    "        curr_y[train_labels[idx]] = 1\n",
    "        feed_forward(x, W, S) \n",
    "        deltas[num_layers] = 2 * (X[num_layers][1:] - curr_y) * sigmoid_prime(S[num_layers])\n",
    "        grads[num_layers] = np.matmul(X[num_layers-1], deltas[num_layers].T)\n",
    "        back_prop(deltas, X, S)\n",
    "        for l in range(1,len(d)):\n",
    "            W[l] = W[l] - eta * grads[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Stochastic Gradient Descent (takes time to run)\n",
    "\n",
    "This function takes around 1 hour to run on my machine. Again, there will inefficiences as this was coded from scratch. A package like tensorflow or keras would be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halfway through.\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(W, S, deltas, eta=0.1, iterations=10000000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weights ##\n",
    "Let's save the weights for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"weights.dict\", W)\n",
    "# read_dictionary = np.load('weights.dict.npy').item()\n",
    "# W = read_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Deep Neural Network\n",
    "Overall, we see a 6.08% training error and a 6.83 test error! So it classified most images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.0608\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(len(train_images)):\n",
    "    curr_x = np.append([1], train_images[i].flatten())\n",
    "    curr_y = train_labels[i]\n",
    "    if np.argmax(h(curr_x)) == curr_y:\n",
    "        total += 1\n",
    "print(\"Training Error:\",  (len(train_images) - total)/len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 0.0683\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "incorrect_idx = []\n",
    "for i in range(len(test_images)):\n",
    "    curr_x = np.append([1], test_images[i].flatten())\n",
    "    curr_y = test_labels[i]\n",
    "    if (np.argmax(h(curr_x))) == curr_y:\n",
    "        total +=1\n",
    "    else:\n",
    "        incorrect_idx.append(i)\n",
    "print(\"Test Error:\",  (len(test_images) - total)/len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting an incorrect Image ##\n",
    "Here, we see that the deep neural network chose to label this image as a 9 instead of a 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13bd8af98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsiw13UeqhA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13bce7a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 100\n",
    "plt.imshow(test_images[incorrect_idx[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[incorrect_idx[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(h(np.append([1], test_images[incorrect_idx[i]].flatten())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
